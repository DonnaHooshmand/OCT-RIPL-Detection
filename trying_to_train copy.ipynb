{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from resources.plotcm import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "from pytorch_datagen import DataGen\n",
    "from resunetPlusPlus_pytorch_copy import build_resunetplusplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train steps:  3100\n",
      "valid steps:  12\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m\n\u001b[1;32m     66\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     68\u001b[0m preds \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m---> 70\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(preds, labels)\n\u001b[1;32m     71\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     72\u001b[0m optimizer\u001b[39m.\u001b[39mstep( )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 4"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ## Path\n",
    "    file_path = \"files/\"\n",
    "    model_path = \"files/resunetplusplus.h5\"\n",
    "\n",
    "    ## Create files folder\n",
    "    try:\n",
    "        os.mkdir(\"files\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_path = \"new_data/kvasir_segmentation_dataset/train/\"\n",
    "    valid_path = \"new_data/kvasir_segmentation_dataset/valid/\"\n",
    "\n",
    "    ## Training\n",
    "    train_image_paths = glob(os.path.join(train_path, \"images\", \"*\"))\n",
    "    train_mask_paths = glob(os.path.join(train_path, \"masks\", \"*\"))\n",
    "    train_image_paths.sort()\n",
    "    train_mask_paths.sort()\n",
    "\n",
    "    # train_image_paths = train_image_paths[:2000]\n",
    "    # train_mask_paths = train_mask_paths[:2000]\n",
    "\n",
    "    ## Validation\n",
    "    valid_image_paths = glob(os.path.join(valid_path, \"images\", \"*\"))\n",
    "    valid_mask_paths = glob(os.path.join(valid_path, \"masks\", \"*\"))\n",
    "    valid_image_paths.sort()\n",
    "    valid_mask_paths.sort()\n",
    "    \n",
    "    ## Parameters\n",
    "    image_size = 256\n",
    "    batch_size = 8\n",
    "    lr = 1e-4\n",
    "    epochs = 200\n",
    "    \n",
    "    train_steps = len(train_image_paths)//batch_size\n",
    "    print(\"train steps: \", train_steps)\n",
    "    valid_steps = len(valid_image_paths)//batch_size\n",
    "    print(\"valid steps: \", valid_steps)\n",
    "    \n",
    "    train_gen = DataGen(image_size, train_image_paths, train_mask_paths)\n",
    "    valid_gen = DataGen(image_size, valid_image_paths, valid_mask_paths)\n",
    "    \n",
    "    ## Turn the data into a torch.utils.data thing\n",
    "    train_loader = torch.utils.data.DataLoader(train_gen, batch_size=8)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_gen, batch_size=8)\n",
    "    \n",
    "    ## ResUnet++\n",
    "    model = build_resunetplusplus()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    \n",
    "    breakpoint()\n",
    "    # The training loop\n",
    "    for epoch in range(1):\n",
    "        total_correct = 0\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(torch.float)\n",
    "            labels = labels.to(torch.float)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            images = images.unsqueeze(1)\n",
    "    \n",
    "            preds = model(images)\n",
    "\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step( )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "        print('epoch:', epoch, \"total_correct:\", total_correct, \"loss:\", total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
