{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from resources.plotcm import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "from pytorch_datagen import DataGen\n",
    "from resunetPlusPlus_pytorch_copy import build_resunetplusplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train steps:  3100\n",
      "valid steps:  12\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n",
      "preds:  torch.Size([8, 1, 256, 256])\n",
      "images:  torch.Size([8, 1, 256, 256])\n",
      "labels:  torch.Size([8, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mimages: \u001b[39m\u001b[39m\"\u001b[39m, images\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     71\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlabels: \u001b[39m\u001b[39m'\u001b[39m, labels\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 72\u001b[0m preds \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     73\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpreds: \u001b[39m\u001b[39m'\u001b[39m, preds\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     74\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(preds, labels)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/resunetPlusPlus_pytorch_copy.py:183\u001b[0m, in \u001b[0;36mbuild_resunetplusplus.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    180\u001b[0m b1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb1(c4)\n\u001b[1;32m    182\u001b[0m d1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md1(c3, b1)\n\u001b[0;32m--> 183\u001b[0m d2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md2(c2, d1)\n\u001b[1;32m    184\u001b[0m d3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md3(c1, d2)\n\u001b[1;32m    186\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maspp(d3)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/resunetPlusPlus_pytorch_copy.py:150\u001b[0m, in \u001b[0;36mDecoder_Block.forward\u001b[0;34m(self, g, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, g, x):\n\u001b[0;32m--> 150\u001b[0m     d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma1(g, x)\n\u001b[1;32m    151\u001b[0m     d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup(d)\n\u001b[1;32m    152\u001b[0m     d \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([d, g], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/resunetPlusPlus_pytorch_copy.py:137\u001b[0m, in \u001b[0;36mAttention_Block.forward\u001b[0;34m(self, g, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m x_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_conv(x)\n\u001b[1;32m    136\u001b[0m gc_sum \u001b[39m=\u001b[39m g_pool \u001b[39m+\u001b[39m x_conv\n\u001b[0;32m--> 137\u001b[0m gc_conv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgc_conv(gc_sum)\n\u001b[1;32m    138\u001b[0m y \u001b[39m=\u001b[39m gc_conv \u001b[39m*\u001b[39m x\n\u001b[1;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Documents/_northwestern/_MSAI/c3 lab/OCT-RIPL-Detection/oct_ripl_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ## Path\n",
    "    file_path = \"files/\"\n",
    "    model_path = \"files/resunetplusplus.h5\"\n",
    "\n",
    "    ## Create files folder\n",
    "    try:\n",
    "        os.mkdir(\"files\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train_path = \"new_data/kvasir_segmentation_dataset/train/\"\n",
    "    valid_path = \"new_data/kvasir_segmentation_dataset/valid/\"\n",
    "\n",
    "    ## Training\n",
    "    train_image_paths = glob(os.path.join(train_path, \"images\", \"*\"))\n",
    "    train_mask_paths = glob(os.path.join(train_path, \"masks\", \"*\"))\n",
    "    train_image_paths.sort()\n",
    "    train_mask_paths.sort()\n",
    "\n",
    "    # train_image_paths = train_image_paths[:2000]\n",
    "    # train_mask_paths = train_mask_paths[:2000]\n",
    "\n",
    "    ## Validation\n",
    "    valid_image_paths = glob(os.path.join(valid_path, \"images\", \"*\"))\n",
    "    valid_mask_paths = glob(os.path.join(valid_path, \"masks\", \"*\"))\n",
    "    valid_image_paths.sort()\n",
    "    valid_mask_paths.sort()\n",
    "    \n",
    "    ## Parameters\n",
    "    image_size = 256\n",
    "    batch_size = 8\n",
    "    lr = 1e-4\n",
    "    epochs = 200\n",
    "    \n",
    "    train_steps = len(train_image_paths)//batch_size\n",
    "    print(\"train steps: \", train_steps)\n",
    "    valid_steps = len(valid_image_paths)//batch_size\n",
    "    print(\"valid steps: \", valid_steps)\n",
    "    \n",
    "    train_gen = DataGen(image_size, train_image_paths, train_mask_paths)\n",
    "    valid_gen = DataGen(image_size, valid_image_paths, valid_mask_paths)\n",
    "    \n",
    "    ## Turn the data into a torch.utils.data thing\n",
    "    train_loader = torch.utils.data.DataLoader(train_gen, batch_size=8)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_gen, batch_size=8)\n",
    "    \n",
    "    ## ResUnet++\n",
    "    model = build_resunetplusplus()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    \n",
    "    breakpoint()\n",
    "    # The training loop\n",
    "    for epoch in range(1):\n",
    "        total_correct = 0\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(torch.float)\n",
    "            labels = labels.to(torch.float)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            images = images.unsqueeze(1)\n",
    "            labels = torch.permute(labels, (0,3,1,2))\n",
    "\n",
    "            preds = model(images)\n",
    "            \n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step( )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += preds.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "        print('epoch:', epoch, \"total_correct:\", total_correct, \"loss:\", total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
