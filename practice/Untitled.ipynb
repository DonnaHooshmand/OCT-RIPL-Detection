{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a04b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from resources.plotcm import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_datagen import DataGen\n",
    "from pytorch_datagen import *\n",
    "from resunetPlusPlus_pytorch_copy import build_resunetplusplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39dcbac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for name, param in model.named_parameters():\n",
    "#    print(\"name: \", name, \" param: \", param, \" requires_grad: \", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dbfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22dbbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available:  True\n",
      "test steps:  100\n",
      "here:  torch.Size([1, 256, 256])\n",
      "here 2:  torch.Size([1, 1, 256, 256])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m predict_mask \u001b[38;5;241m=\u001b[39m (predict_mask \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     61\u001b[0m sep_line \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((image_size, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m---> 63\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_to_3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m predict_mask \u001b[38;5;241m=\u001b[39m mask_to_3d(predict_mask)\n\u001b[0;32m     66\u001b[0m all_images \u001b[38;5;241m=\u001b[39m [image \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, sep_line, mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, sep_line, predict_mask]\n",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m, in \u001b[0;36mmask_to_3d\u001b[1;34m(mask)\u001b[0m\n\u001b[0;32m      2\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(mask)\n\u001b[0;32m      3\u001b[0m mask \u001b[38;5;241m=\u001b[39m [mask, mask, mask]\n\u001b[1;32m----> 4\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:660\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    Reverse or permute the axes of an array; returns the modified array.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m \n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranspose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "def mask_to_3d(mask):\n",
    "    mask = np.squeeze(mask)\n",
    "    mask = [mask, mask, mask]\n",
    "    mask = np.transpose(mask, (1, 2, 0))\n",
    "    return mask\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"ColonoscopyTrained_resUnetPlusPlus.pkl\"\n",
    "    save_path = \"result\"\n",
    "    test_path = \"../new_data/kvasir_segmentation_dataset/test/\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"GPU available: \", torch.cuda.is_available())\n",
    "\n",
    "    image_size = 256\n",
    "    batch_size = 1\n",
    "\n",
    "    test_image_paths = glob(os.path.join(test_path, \"images\", \"*\"))\n",
    "    test_mask_paths = glob(os.path.join(test_path, \"masks\", \"*\"))\n",
    "    test_image_paths.sort()\n",
    "    test_mask_paths.sort()\n",
    "\n",
    "    ## Create result folder\n",
    "    try:\n",
    "        os.mkdir(save_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ## Model\n",
    "    model = build_resunetplusplus()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "      \n",
    "    test_steps = len(test_image_paths)//batch_size\n",
    "    print(\"test steps: \", test_steps)\n",
    "    test_gen = DataGen(image_size, test_image_paths, test_mask_paths)\n",
    "    test_loader = torch.utils.data.DataLoader(test_gen)\n",
    "    \n",
    "    #switch off autograd\n",
    "    with torch.no_grad():\n",
    "        #det the model in evaluation mode\n",
    "        model.eval()\n",
    "        for batch in test_loader:\n",
    "            images, masks = batch\n",
    "            \n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            masks = masks.to(device, dtype=torch.float)\n",
    "            print(\"here: \", images.size())\n",
    "            \n",
    "            images = images.unsqueeze(1).to(device)\n",
    "            print(\"here 2: \", images.size())\n",
    "            masks = masks.permute(0, 3, 1, 2).to(device)\n",
    "            \n",
    "            predict_mask = model(images).to(device)\n",
    "            \n",
    "            loss = F.mse_loss(predict_mask, masks).to(device)\n",
    "            \n",
    "            \n",
    "            predict_mask = (predict_mask > 0.5) * 255.0\n",
    "            \n",
    "            sep_line = np.ones((image_size, 10, 3)) * 255\n",
    "            \n",
    "            mask = mask_to_3d(masks)\n",
    "            predict_mask = mask_to_3d(predict_mask)\n",
    "            \n",
    "            all_images = [image * 255, sep_line, mask * 255, sep_line, predict_mask]\n",
    "            cv2.imwrite(f\"{save_path}/{i}.png\", np.concatenate(all_images, axis=1))\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "895640bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_datagen.DataGen"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d0393ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_path = \"new_data/kvasir_segmentation_dataset/test/\"\n",
    "test_image_paths = glob(os.path.join(test_path, \"images\", \"*\"))\n",
    "test_image_paths.sort()\n",
    "test_steps = len(test_image_paths)\n",
    "print(test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfbb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
